
üîπ Introdu√ß√£o
esse aqui √© um v√≠deo muito especial que
a gente preparou para voc√™s contendo
toda a informa√ß√£o que o nosso time
conseguiu juntar sobre o Deep seic o
modelo de linguagem aberto chin√™s que
foi lan√ßado recentemente respons√°vel por
fazer o Mercado de A√ß√µes de Big Tex
norte-americano perder mais de 1 trilh√£o
de valor de mercado em apenas um dia e
at√© o final dessa masterclass voc√™ vai
ter entendido Por que que esse modelo √©
t√£o especial e Que mudan√ßas ele vai
trazer pro mercado de Intelig√™ncia
Artificial daqui pra frente voc√™ vai
entender a princip qual diferen√ßa entre
o modelo Deep seic V3 que √© uma vers√£o
mais tradicional comum parecida com o
que a gente utiliza todos os dias e o
rsc R1 que √© o respons√°vel por fazer o
mercado ter todo esse reboli√ßo que seria
um modelo de racioc√≠nio vai entender
como utilizar ele no teu navegador e
aproveitar ao m√°ximo dessa
funcionalidade de pensamento expl√≠cita
que esse modelo tem e a parte que eu
julgo mais importante eu vou ensinar
voc√™s a como rodar o d psic R1 e os
modelos que foram treinados a partir
deles diretamente da sua m√°quina de
maneira 100% offline e sem censura Ent√£o
==================================================

üîπ Uma breve retrospectiva
bora l√° todo esse rebuli√ßo do mercado
come√ßou aqui no dia 26 de dezembro posso
dizer quando foi apresentado o dips V3
que ele √© um modelo de linguagem com
caracter√≠sticas para n√≥s usu√°rios
semelhante ao que seria chat GPT 4 o ou
um Cloud 3.5 sonet no sentido de que a
gente faz perguntas e obt√©m uma resposta
Direta do modelo e a parte interessante
√© que esse modelo foi apresentado com
uma quantidade de par√¢metros
relativamente m√©dia uma arquitetura n√£o
t√£o comum que a gente chama de mixture
of experts diferente de modelos
tradicionais aqui como o lama ou quen
que tem uma arquitetura densa e
acredita-se que o chat GPT 4 o e o clou
3.5 Sony tamb√©m tem um arquiteturas como
essa mas n√£o se sabe porque s√£o empresas
fechadas e ele apresentou aqui bench
Marks que s√£o na maioria das vezes at√©
superiores aos principais modelos de
mercado aqui o da o cloud 3.5 e o 4 o
mas a parte interessante aqui que esse
modelo ele √© de c√≥digo fonte aberto e
foi treinado de uma forma bem diferente
alguns dias depois eles tamb√©m lan√ßaram
um aplicativo nas principais app Stores
do mundo mas o reboli√ßo mesmo do mercado
==================================================

üîπ Lan√ßamento do DeepSeek R1
aconteceu aqui no dia 20 de janeiro
quando eles lan√ßaram o modelo R1 o R1 √©
uma llm que se compara ao modelo mais
poderoso do mundo que a gente tinha
consci√™ncia at√© agora que era o open ai
all one Qual que √© a diferen√ßa quando a
gente usa o chat GPT normalmente aqui no
modelo 4 o eu fa√ßo uma pergunta para ele
ele automaticamente me d√° uma resposta
ele come√ßa a Gerar tokens de sa√≠da a
partir daqui ent√£o se eu pe√ßo para ele
aqui Escreva um c√≥digo em Python para
resolver equa√ß√£o de basca ele
automaticamente come√ßa a me dar as
respostas aqui diretamente no prompt o
modelo onean de racioc√≠nio ele vai
introduzir uma etapa de reflex√£o sobre
aquele problema antes de nos dar as
resposta ent√£o reparem que ele n√£o t√°
escrevendo mas ele est√° pensando
anteriormente essa aqui foi uma t√©cnica
desenvolvida pela Open a para poder
resolver problemas que os outros modelos
estavam falhando por exemplo de
desenvolver c√≥digo melhor algumas
equa√ß√µes de matem√°tica problemas Gerais
aqui que envolviam um racioc√≠nio mais
profundo racioc√≠nio l√≥gico onde os os
modelos tinham muita dificuldade
cometiam erros at√© b√°sicos que n√≥s seres
humanos n√£o cometeriam e at√© esse
momento o o1 era dominante nessa
categoria s√≥ que ele tinha um s√©rio
problema ele era extremamente caro para
voc√™s terem uma ideia que rodar o o1
custava $0 para cada milh√£o de tokens de
sa√≠da em compara√ß√£o aqui com 4 o que
custava seis vezes menos e o paper que o
deeps lan√ßou ele aponta algumas mudan√ßas
na forma como esses modelos foram
treinados na pr√≥pria arquitetura e eles
conseguiram atingir esses mesmos
resultados gastando uma pequena fra√ß√£o
do que a gente acreditaria que seria
necess√°rio para treinar um modelo top de
linha como o owan e o fato desse modelo
ter atingido uma performance semelhante
a all one custando 50 vezes menos o
deeps que ainda ter dominado o ranking
de downloads na Apple Store em v√°rios
pa√≠ses nos √∫ltimos dias fez com que as
a√ß√µes do mercado americano abrissem uma
queda bizarra por exemplo a NV aqui na
segunda-feira do dia 27 abriu em queda
de mais de 10% ent√£o o mercado come√ßou a
pensar cara talvez n√£o seja necess√°rio
==================================================

üîπ O futuro pr√≥ximo das IAs
todo esse investimento bizarro que t√°
sendo feito em placas de v√≠deo para
treinar modelos t√£o poderosos porque uma
galera l√° na China com muito menos
capital conseguiu ter resultados
semelhantes ao que a gente tem feito
aqui gastando muito menos ent√£o o
mercado pensou talvez n√£o seja
necess√°rio fazer esses investimentos
bizarros que TM sido feitos em data
Centers com placas de v√≠deo de ponta e s
focar em outras arquiteturas mais
criativas de ganho de efici√™ncia para
atingir esses mesmos resultados √© isso
que o R1 simboliza √© por ISO por isso
que ele Dominou a m√≠dia por isso que t√°
chamando tanta aten√ß√£o Eu at√© j√°
comentei isso num v√≠deo passado aqui do
canal para quem j√° assistiu sobre uma
vis√£o que eu fiz h√° se meses atr√°s sobre
o que que eu acreditava que era o futuro
de curto prazo das intelig√™ncias
artificiais e se a gente for colocar em
retrospectiva primeiramente a gente tem
que lembrar que houve o lan√ßamento l√°
atr√°s do chat GPT 3.5 seguido pelo chat
GPT 4 que apresentava um ganho de
performance muito melhor por√©m o quatro
Era um modelo muito muito muito caro pra
√©poca e quando pt4 o foi lan√ßado ele
apresentou duas caracter√≠sticas
importantes primeiro ele era um modelo
multimodal ou seja ele tinha n√£o s√≥
capacidade de gerar texto mas entender
imagens de de gerar √°udio Teoricamente
ele conseguia se comunicar e lidar com
diversos tipos de entradas de dados ao
mesmo tempo ent√£o multimodalidade era um
fator mas o segundo que acho que s√≥ os
programadores prestaram mais aten√ß√£o √© o
fato dele ser um modelo muito mais
barato do que vers√£o 4 o na na apepi por
exemplo era praticamente bizarro
impag√°vel a diferen√ßa entre os dois e o
pr√≥prio modelo 4 S√≥ estava dispon√≠vel
para quem pagava na vers√£o Pro coisa que
um 4 o hoje a gente at√© consegue acessar
de maneira gratuita e l√° atr√°s eu fiz
uma proje√ß√£o tamb√©m falando que as
arquiteturas dos modelos de linguagem na
verdade elas por mais que sejam
incr√≠veis estejam conseguindo reproduzir
o comportamento humano a fala e mesmo
absorver muito muito muito conhecimento
elas s√£o absurdamente ineficientes Ou
seja √© necess√°rio fazer um modelo de
linguagem treinar por milhares de horas
em placas de v√≠deos car√≠ssimas processar
a√≠ terab de informa√ß√£o baixados de toda
a internet para conseguir chegar nesse
modelo comprimido que repete o que a
gente fala ou seja existe muita muita
muita muita margem para ganho de
efici√™ncia em reposicionar os elementos
na pr√≥pria arquitetura e esse √© ainda o
que eu acredito que seja o movimento
futuro daqui pra frente ganho de
efici√™ncia
downscale cada vez eu acredito mais em
modelos bons como a gente tem um 4 o em
tamanhos cada vez menores eu n√£o julgo
t√£o importante a gente ganhar mais
qualidade nos modelos e sim a gente
conseguir ter acesso a modelos menores
que daqui a pouco n√≥s vamos ter no nosso
pr√≥prio computador aqui a capacidade de
localmente estar rodando llms de ponta
processando no hardware f√≠sico local sem
ter que estar consumindo api das
bigtechs eu acredito tamb√©m que a
pr√≥xima gera√ß√£o de computadores a gente
j√° v√™ alguns ser lan√ßado com isso v√£o
conter chips otimizados para rodar
modelos de linguagem a pr√≥pria Microsoft
tem lan√ßado alguns recentemente com as
suas npu e a minha previs√£o √© que n√≥s
vamos ter cada vez mais computadores
sendo lan√ßados com capacidade de rodar
esses modelos locais e cada vez mais
modelos atingindo resultados top com
menos par√¢metros √© isso que eu espero
inclusive Esse √© um dos motivos que fez
a gente aqui investir numa m√°quina local
como essa que eu j√° fiz v√≠deo aqui no
canal para rodar modelos de a porque eu
t√¥ esperando ansiosamente pelo dia que
eu vou ter sei l√° um de psic v4 um de
psic V5 com 10 bilh√µes de par√¢metros os
15 bilh√µes de par√¢metros ou um lama 5 um
==================================================

üîπ Modelos Destilados de IA
lama 6 com uma qualidade compar√°vel ao 4
de hoje em dia que √© excelente ent√£o
beleza A gente j√° entendeu a diferen√ßa
entre o V3 e o R1 mas no paper que
descreve o R1 eles tamb√©m lan√ßaram uma
outra caracter√≠stica que eu acho que o
mercado tamb√©m n√£o t√° prestando muita
aten√ß√£o eu t√¥ que seriam esses modelos
distilled que tem tudo a ver com o que
eu falei agora e o que que eles fizeram
tem um gr√°fico pequeno aqui de um v√≠deo
que eu achei muito legal da galera do L
Chen que explica esse processo que seria
o seguinte o R1 ele √© derivado de um
processo pode-se dizer assim de uma
esp√©cie de fine tuning do V3 Aonde eles
inserem uma cadeia de pensamentos
diversas cadeias de pensamento para
permitir que ele consiga pensar e
refletir antes de dar a sua resposta e a
gente j√° vai ver isso na pr√°tica quando
for rodar o dpsc aqui no navegador e
diversas dessas cadeias de fine tun
foram inseridas aqui no meio para
permitir que o modelo consiga raciocinar
mas essa curvinha roxa aqui √© o que eu
acho mais legal cara essa curvinha roxa
eles Peg Aram diversos modelos
min√∫sculos s√£o modelos aqui como o lama
de 8 bilh√µes de par√¢metros lama de 70 o
quen de 1.5 7 14 32 que s√£o modelos que
rodam em m√°quinas locais rodam nesse meu
computador aqui sem eu ter que ter sem
eu ter que usar nada n um acesso √†
internet e o nosso computador aqui de
adas im√≥ve roda esses caras aqui em
velocidade impressionante e o que que
eles fizeram Eles colocaram o R1 para
treinar estes modelos pequenos aqui a
adquirir capacidade de racioc e eles
chamaram isso de de psic R1 distilled
models ou seja agora n√≥s tamb√©m temos
modelos pequenos com capacidade de
racioc√≠nio ent√£o reparem aqui embaixo
nesse paper que a gente tem diversos
desses modelos s√£o treinados ent√£o por
exemplo R1 detil Queen 1.5b que seria
uma um modelo distilled aqui do modelo
original que √© o Queen de 1.5 bilh√µes de
par√¢metros t√° modelo de 7 de 14 32 ou
Lhama de 8 de 70 e os benchmarks eles
s√£o interessantes √≥ porque pra tarefas
que s√£o ligadas a racioc√≠nio esses
modelos pequenos como por exemplo um
Queen de 7 bilh√µes de par√¢metros que √©
um modelo min√∫sculo min√∫sculo ele
consegue resultados melhores do que o
chat GPT 4 olha que curioso ele √©
compar√°vel com o modelo mais mais um dos
mais poderosos da da da Open a que n√£o
raciocina Claro mas eles tamb√©m
conseguiram resultar ados principalmente
vers√£o de 14 de 32 bilh√µes de par√¢metros
aqui compar√°vel com o1 mini que √© um
modelo a vers√£o pequena da Open a a mais
barata assim do de racioc√≠nio que √© um
modelo muito poderoso isso aqui para mim
eu acho incr√≠vel e eu vou ensinar voc√™s
no final desse v√≠deo como rodar esses
caras aqui localmente a segunda pergunta
==================================================

üîπ Quais os pre√ßos dos DeepSeek?
que fica √© qu√£o mais barato √© o DPC
comparado com chat GPT por exemplo eu
posso dar duas respostas para isso uma
se voc√™s est√£o no n√≠vel de usu√°rio ou se
est√£o no n√≠vel de programador no n√≠vel
de usu√°rio at√© esse momento o dpsc ele √©
totalmente gratuito basta voc√™s entrarem
aqui no dpsc criarem uma conta podem
come√ßar a utilizar o modelo tanto o
tradicional que √© o V3 quanto o dips R1
clicando nesse bot√£o e ele tem inclusive
at√© uma uma funcionalidade aqui de
procurar procurar na Internet isso aqui
√© tudo gratuito pra gente poder ter o
mesmo n√≠vel de acesso no chat GPT √©
necess√°rio pagar a√≠ 20 por m√™s que t√°
por volta de uns R 120 a gente teria
acesso ilimitado ao 4 seria semelhante
ao dips V3 sem o bot√£ozinho apertado ou
eu poderia clicar aqui no o1 e teria
compar√°vel ao R1 do dsic Ent√£o essa √© a
diferen√ßa entre usu√°rios para usu√°rios
comuns de gra√ßa contra 0 por m√™s para
usu√°rios mais avan√ßados que querem
utilizar esses modelos de linguagem na
Api para construir assistentes
personalizados agentes usando
programa√ß√£o como √© o que a gente faz o
que a gente vive defendendo que voc√™s
tamb√©m aprendam a construir a gente vai
est interessado em outra tabela que
seria a tabela de pricing das apis t√°
ent√£o no deeps dessa forma N√≥s temos
dois pre√ßos diferentes a gente pode
utilizar api direta pagando estes cursos
aqui que eu j√° vou explicar Ou a gente
pode baixar o modelo porque ele √© open
source gratuito instalar num servidor
nosso ou no servidor que a gente alugue
E a√≠ os custos do modelo √© gratuito mas
a gente vai ter que pagar pela
infraestrutura s√£o essas duas op√ß√µes
e a openai a gente s√≥ tem a op√ß√£o de
pagar pela api deles qual a diferen√ßa de
pre√ßo Ent√£o vou comparar aqui o que que
seria o compar√°vel Deep 6 chat que seria
o V3 est√° aqui ele t√° custando a√≠ 1
centavo de D√≥lar para cada 1 milh√£o de
tokens de entrada isso aqui √© muito
muito muito barato se eu for comparar
por exemplo com o 4 ou que √© o o par
dele √© 1 centavo contra 2 por milh√£o de
tokens de entrada Olha a diferen√ßa disso
aqui t√° √© 1 centavo contra 50 250 125 na
verdade 125 vezes mais barato em
compara√ß√£o aqui com 10 por milh√£o de
tokens de sa√≠da esse tokens de entrada
aqui galera √© o prompt que a gente manda
ele precisa primeiro processar esse
prompt existe um pre√ßo diferente entre
tokens de entrada e tokens de sa√≠da para
ele gerar gerar um pouco mais caro do
que processar os tokens t√° √© um processo
mais Custoso mas aqui ele custa 0 para
gerar 1 milh√£o de tokens comparado com
28 centos para gerar 1 milh√£o de tokens
ent√£o uma diferen√ßa a√≠ de 40 vezes mais
ou menos no modelo de racioc√≠nio a coisa
fica ainda mais absurda a diferen√ßa a
gente tem 14 centavos de D√≥lares de
entrada no modelo de racioc√≠nio e √≥ no
modelo de sa√≠da t√° ent√£o o modelo de
sa√≠da ainda assim √© mais barato que o da
da Open a 4 o s√≥ que o modelo o1 que √©
esse que eu comentei que √© o mais
poderoso do mundo hoje o caro ele t√°
custando $0 de sa√≠da ent√£o a diferen√ßa √©
2 Contra 0es tem at√© esse post aqui que
eu tava dando uma lida antes pessoal
comparando essa diferen√ßa entre os dois
e dizendo que √© absurda nesse momento √©
um grande cheque que esse modelo coloca
em rela√ß√£o a essas bigtech j√° estavam
consolidadas no mercado como Open ai e
antropic como √© que elas v√£o lidar com
esses modelos novos que est√£o chegando
agora ent√£o beleza eu j√° fiz uma an√°lise
de mercado dei meu Pitaco em rela√ß√£o ao
futuro falamos de pricing Vamos explorar
==================================================

üîπ Explorando o modelo
um pouquinho esse modelo aqui entender
um pouco das suas diferen√ßas ent√£o para
quem j√° estava acostumado com o modelo
de pensamento da openi j√° era comum
mandar algum prompt qualquer como por
exemplo Como saber se n√£o viemos em uma
simula√ß√£o ele vai pensar um pouco sobre
isso uma quest√£o at√© um pouco filos√≥fica
aqui e essa resposta a gente n√£o sabe
com clareza o que t√° acontecendo porque
a Open ai tem de tudo menos aberta a
gente de fato n√£o sabe como os modelos
funcionam n√£o tem acesso √† quantidade de
par√¢metros ou a estrutura deles mas a
teoria o que a gente imagina que possa
estar acontecendo √© que o modelo ele usa
um Framework de conversar consigo mesmo
de se auto prompt vamos dizer assim para
poder chegar nessa resposta final o de
psic ele tem uma diferen√ßa quando eu
mando uma pergunta como essa a gente
pode clicar aqui e ver o processo de
pensamento ou seja Pensar faz parte do
processo de gera√ß√£o de prompt dele isso
aqui vai ficar bem expl√≠cito quando eu
mostrar esse modelo aqui sendo gerado na
minha m√°quina local ele tem claramente
assim o que √© pensamento e o que que n√£o
√© isso aqui √© bem interessante porque a
gente pode utilizar essa habilidade aqui
em alguns casos para entender como
pensar sobre algum problema e eu j√° vou
mostrar alguns exemplos daqui a
pouquinho outra quest√£o interessante
aqui √© que ele pensou em ingl√™s e me deu
a resposta na l√≠ngua original portugu√™s
e ele vai propor aqui uma grande
resposta Ele pensou por volta de uns 30
segundos aqui para para gerar isso agora
vamos fazer um teste com alguns modelos
eu vou colocar aqui no chat GPT 4 pedir
para ele escreva um jogo de corrida em
Python usando trnl esse prompt aqui ele
tem uma pegadinha porque streamlit em
geral uma biblioteca boa para construir
dashboards pra gente analisar dados ela
n√£o √© muito comum para desenvolver jogos
e eu deixei o que que seria esse jogo √©
um jogo em aberto ele vai chegar na
l√≥gica pr√≥pria dele sobre o que que √© um
jogo ou n√£o e eu quero ver se o chat GPT
4 ou na vers√£o tradicional dele sem
pensar ele consegue se ligar que o
stramit n√£o √© muito bom para jogo e
mesmo assim desenvolver alguma coisa que
seja algum jogo que fa√ßa sentido que
seja jog√°vel ent√£o eu vou com comparar
chat chpt 4 o contra o chat chpt all one
==================================================

üîπ Fazendo um jogo com Streamlit
contra o Deep seic R1 n√≥s vamos ver a
diferen√ßa do sistema gerado por cada um
deles se eles conseguem gerar alguma
coisa rod√° Vel E se o jogo em si faz
sentido vamos l√° ent√£o o 4 j√° est√°
gerando aqui enquanto isso fazer a mesma
pergunta pro w One ele vai pensar um
pouco n√£o sei o processo de pensamento
dele e eu vou pedir a mesma coisa pro R1
sempre lembrem de marcar aqui embaixo
ele vai pensar e aqui vem uma parte
interessante desse m√©todo de pensamento
reparem aqui que ele pegou a pegadinha
ele at√© comentou aqui primeiro eu sei
que string √© um Framework para an√°lise
de dados Mas ser√° que ele pode lidar com
jogos ent√£o voc√™s v√£o perceber que a
resposta dele Eu imagino J√° consiga ter
essa no√ß√£o de que ele vai ter que fazer
algo para poder passar por cima disso
aqui j√° t√° dando a resposta beleza j√°
tem um jogo aqui do 4 o deixa eu rodar
esse cara ent√£o t√° aqui o c√≥digo n√£o
apontou erros e n√£o funcionou aplica√ß√£o
teve algum bug aqui que ele n√£o
funcionou ent√£o 4 n√£o passou nesse meu
teste vamos ver a vers√£o agora do all
one all one tamb tamb√©m n√£o n√£o estragou
aqui vou rodar t√° vamos ver se esse √© um
jogo eu tenho que ficar apertando bot√µes
√≥ pelo visto √© um jogo bem ruim √≥ t√° que
eu posso ir dirigindo ali o meu carro
aquilo ali √© um carro pelo visto √© o
jogo mais f√°cil do mundo isso aqui s√£o
obst√°culos t√° ent√£o isso aqui ser de
carro cara n√£o esperava n√£o esperava
para um jogo como
esse √© um jogo muito
ruim mas t√° valeu passou no teste vamos
ver o o jogo agora do debs R1 √≥ cara
esse aqui √© um jogo de verdade √≥
funcionou √≥ realmente foi dif√≠cil √≥ tem
um caminho ele t√° iterando sozinho √≥ eu
consigo desviar dos obst√°culos √ì fa√ßo
pontos em fun√ß√£o de quanto tempo vou
passando foi foi um jogo legal funcionou
de fato foi at√© legal ver Porque de fato
ele produziu uma coisa que claramente
superior ao a1 e n√£o √© um jogo simples
t√° Por mais que seja bem bobo aqui n√£o √©
um jogo simples de da gente conseguir
gerar de primeira excelente Ent√£o esse
exemplo aqui n√£o √© para fazer nenhum
benchmark entre os modelos provar que um
√© melhor que o outro porque ele
conseguiu gerar um o jogo melhor √© um
teste bem complexo de ser feito eu teria
que testar sobre in√∫meras in√∫meros
benchmarks diferentes sobre √≥ticas
diferentes argumentos diferentes mas o
Ponto Central aqui √© voc√™s entenderem as
principais diferen√ßas entre esse modelo
e o modelo tradicional da Open a mas
agora tem uma coisa curiosa a usar o
modelo no chat que ele tem censura em
rela√ß√£o √† pr√≥pria China ent√£o se eu
perguntar aqui para ele por exemplo a
China vive em uma ditadura reparem que
ele vai pensar um pouco √≥ t√° perguntando
sobre a China vamos ver se ele vai me
dar uma resposta desculpe eu n√£o posso
ah lidar com esse tipo de pergunta vamos
falar sobre matem√°tica coding l√≥gica e
problemas ou seja ele detectou Em algum
momento ali que haveria alguma coisa em
rela√ß√£o √† censura chinesa e cortou fora
do modelo mas a vers√£o local que a gente
pode baixar que n√£o tem problema quanto
a isso ent√£o √© isso espero que at√© esse
momento voc√™s tenham entendido a
diferen√ßa entre o V3 e o R1 e como rodar
==================================================

üîπ Como funciona o DeepSeek Local
na web mas agora chegou a parte
preferida que √© como √© que a gente faz
para ter acesso de fato a a esse modelo
colocar a m√£o nele rodar localmente uma
forma oficial de fazer isso se voc√™s
digitarem Deep seic R1 github t√° a gente
vai cair no github do Deep seic mesmo de
fato que inclusive j√° t√° com 50.000
estrelas aqui a galera levou para 50.000
estrelas em uma semana ISO que √© uma
velocidade impressionante tem todas as
especifica√ß√µes que eu j√° comentei com
voc√™s sobre os testes de performance
comparado com A1 uma descri√ß√£o do modelo
o que que eles fizeram a quantidade de
par√¢metros quantos par√¢metros s√£o
ativados os modelos destilados que a
gente vai trabalhar daqui a pouquinho e
aqui embaixo tem uma parte que fala
sobre como rodar esses modelos
localmente ele separa aqui entre os
modelos R1 e os modelos detil esse aqui
vai ser um pouco mais f√°cil eu vou
clicar aqui ele me leva pro reposit√≥rio
original do V3 porque lembrem o R1 foi
derivado do V3 e aqui embaixo tamb√©m a
gente tem um novo guia sobre como rodar
isso aqui localmente e aqui ele passa
sete formas diferentes para voc√™s
rodarem o modelo s√£o aplica√ß√µes como por
exemplo LM Deploy √© uma que eu uso
bastante para rodar j√° desenvolvi v√≠deos
aqui no canal sobre isso √© um Framework
aqui que vai permitir que a gente
Execute modelos de linguagem localmente
s√≥ que eu n√£o vou explorar nenhum desses
caras aqui eu vou apresentar para voc√™s
uma forma muito mais f√°cil que √© atrav√©s
do olama que √© um programinha que voc√™s
podem baixar para Windows Mac Linux e
facilita muito o processo de executar os
modelos mas eu tenho uma m√° not√≠cia
vamos dizer assim em rela√ß√£o ao ao V3 ou
R1 porque esses modelos eles possuem a√≠
670 bilh√µes de par√¢metros quem lembra
desse outro v√≠deo aqui do Canal Ae eu
falo sobre como montar um computador
local vai lembrar que eu tamb√©m falo
sobre a rela√ß√£o entre a quantidade de VR
que voc√™s t√™m que ter no computador de
voc√™s e a quantidade de par√¢metros que o
modelo tem em geral a rela√ß√£o fica um
para um quando a gente t√° utilizando um
modelo F8 sem nenhum tipo de compress√£o
que voc√™s teriam que ter quase 700 GB de
vram ou seja mem√≥ria de placa de v√≠deo
para conseguir rodar es carar aqui na
vers√£o pr√≥pria como eles est√£o fazendo e
isso aqui galera √© algo bem complicado
porque vram √© algo bem caro de se obter
bem dif√≠cil de se organizar esse nosso
computador aqui custou quase r$ 2000 a
gente tem s√≥ 48 Gb de vram Eu precisaria
a√≠ de mais 600 GB de vram para conseguir
rodar ele localmente e mesmo assim a
velocidade seria bem ruim algumas
pessoas do Twitter j√° compartilharam
aqui um setup que eles tiveram que fazer
para conseguir rodar o o V3 ou o R1 e
reparem que esse cara aqui ele empilhou
1 2 3 4 5 6 7 8 8 88 Mac minis M4 Pro 64
GB PR conseguir rodar esse cara ele fez
um cluster local t√° Caso voc√™s n√£o
tenham ideia de quanto custa cada um
desses vamos pesquisar aqui t√° a
bagatela a√≠ de
24.000 cada ent√£o voc√™s precisariam
empilhar a√≠ oito desses montar um
sisteminha como que esse cara t√° fazendo
aqui para conseguir rodar porcamente o
V3 t√° Ent√£o olha a velocidade de gera√ß√£o
de tokens ali ele t√° pensando at√© ali
aqui na na esquerda iria aparecer √≥ n√£o
√© nem r√°pido o cara gastou a√≠ R
150.000 para conseguir ter uma
velocidade Meia Boca do que a gente
conseguiria gratuitamente aqui entrando
no entrando no dips ent√£o p√¥ na verdade
isso n√£o √© t√£o bom para n√≥s
desenvolvedores n√© existe uma
alternativa Caso voc√™s queiram rodar o
R1 mesmo completo que seria alugar essas
placas na internet ent√£o por exemplo tem
sites aqui como por exemplo esse que eu
encontrei o runpod existe um outro que
eu j√° falei v√°rias vezes aqui no canal
que √© o ves pai que √© um Global GPU
marketing a gente consegue alugar essas
placas de v√≠deo servidores de outras
pessoas pagando por Demanda ent√£o
poderia vir aqui na internet alugar √≥
uma h200 tem um um deira Center aqui por
aluguel √≥ aqui o cara tem oito placas
h200 que √© uma placa que tem cada uma
dela 140 GB t√° inclusive um setup um
cluster com 8h 200 √© o que a galera do
jeeps recomenda num dos papers que eles
citaram aqui para que a gente consiga
rodar esses caras localmente mesmo t√° $5
por hora √© um c R 50 por hora para voc√™s
conseguirem executar esse modelo ent√£o √©
bem caro sim mas essas s√£o as
alternativas t√° ent√£o o que nos resta
nos resta rodar os modelos destilados
que como eu comentei anteriormente eles
TM performance semelhante ao o1 mini que
√© um modelo bem bom t√° modelo acess√≠vel
e bem bom e para problemas como coding
racioc√≠nio l√≥gico ou √†s vezes escrever
at√© alguns textos esses modelos podem
ter boas performances e cabem em muitos
computadores ent√£o entrando no huging
Face aqui do DPS tem aquela tabelinha
lha onde ele faz uma compara√ß√£o aqui com
por exemplo modelos pequenos 14 bilh√µes
de par√¢metros que Eu precisaria de uma
placa de 14 GB de vram modelos de 7 aqui
que cara caberiam numa placa de 8 Gb tem
muita placa de 8 GB barata por a√≠ a rtx
3050 Se n√£o me engano √© a placa mais
barata a atingir a barreira dos 8 GB
Ent√£o j√° caberia esse modelo l√° e
reparem aqui que pelos benchmarks ele t√°
compar√°vel j√° ao o1 mini sendo que o A1
mini olha aqui √© um modelo car√≠ssimo √≥ √©
um modelo de 12 por milh√£o de tokens t√°
ent√£o ent√£o localmente a gente j√°
consegue rodar esses modelos aqui que
==================================================

üîπ Como rodar o DeepSeek localmente?
s√£o compar√°veis com ele vamos fazer isso
vamos bora l√° primeira coisa que voc√™s
TM que fazer √© entrar no site olham.com
t√° E voc√™s v√£o baixar o olama √© bem
simples o processo para Windows Mac e
Linux ele varia um pouquinho no meu caso
eu j√° baixei ele pra Mac ele fica aqui
em cima que a gente acessa esse cara
atrav√©s de alguns comandos de terminal
bem f√°cil mas hoje eu n√£o vou rodar
neste meu Mac aqui eu vou come√ßar
fazendo uns testes no nosso computad ele
vai utilizar apenas uma rtx 3090 que √©
uma placa de 24 GB de vram eu vou me
conectar com esse servidor atrav√©s de
comando SSH aqui n√£o reparem isso ele √©
um computador que t√° no meu servidor
aqui reparem que eu j√° tenho aqui essas
duas rtx 3090 Cada uma com seus 24 GB de
VR e n√£o est√£o sendo utilizadas Ent√£o
vamos l√° como √© que a gente roda eu
entro aqui no olama entro na parte de
models e clico aqui de psic R1 t√° ele
vai me dar aqui uma s√©rie de op√ß√µes √≥
op√ß√µes de set bilh√µes de 8 bilh√µes de 14
bilh√µes 3270 e o modelo completo que √©
esse que eu falei que a gente quase
quase n√£o consegue rodar eu vou pegar
esse de 14 bilh√µes aqui que tem uma
performance ok ele √© um modelo que t√°
utilizando a arquitetura quen 2 t√° ele
foi √© um fine tuning em cima do quen
como eu comentei na vers√£o de 14 bilh√µes
de par√¢metros e ele t√° passando por uma
quantiza√ß√£o q4 ent√£o esses par√¢metros
originais foram para mais ou menos 1/4
do tamanho que ele teria ent√£o √© um
modelo que pesa 9 GB a gente copia Esse
comando aqui entra no terminal coloca
olama Run Deep Seek se a primeira vez
que voc√™s est√£o rodando ele vai baixar o
modelo e pode demorar um pouquinho e
pronto a gente j√° tem aqui a nossa
mensagem vou perguntar em primeiro lugar
√© verdade que a China vive uma ditadura
e olha que interessante primeira coisa
que ele faz aquela parte de F ele t√°
expl√≠cito aqui √≥ atrav√©s de F ele me
devolve j√° um caracter de F E quando ele
parar de pensar ele vai me dar o
caracter de fechamento e a resposta est√°
aqui embaixo t√° e esse modelo aqui ele
n√£o teve nenhum tipo de censura ent√£o eu
tive a resposta aqui sem ter aquele
banimento que ele tinha me falado antes
por√©m ele n√£o √© um modelo t√£o
inteligente quanto o R1 normal porque eu
fiz uma pergunta aqui em portugu√™s ele
pensou em ingl√™s e esqueceu que a
pergunta tinha sido feita em em
portugu√™s e me devolver uma resposta em
ingl√™s beleza vamos ver a pr√≥xima
pergunta Construa um jogo de corrida no
Python usando string vamos ver o que ele
vai me dar ent√£o l√° vai ele vai me dar
um uma um pensamento aqui em primeiro
lugar quando ele come√ßar a me dar o
c√≥digo vamos ver se ele consegue de fato
me gerar um jogo como a vers√£o R1
conseguiu ent√£o ele parou de pensar aqui
t√° Demorou bastante pensando olha quanto
token ele gerou reparem que no meu
computador o modelo roda bem r√°pido t√°
porque eu t√¥ com com computador feito
para isso quer dizer acess√≠vel n√© n√£o √©
um PC t√£o quanto essas empresas
profissiona as empresas de verdade T mas
uma 3090 √© uma placa pag√°vel t√° terminou
o jogo aqui ent√£o vou copiar ele vamos
ver ver se ele consegue alguma coisa n√©
terminou aqui o c√≥digo em Python e essa
aqui eu vou chamar de local √≥ reparem
que o jogo j√° bugou Deixa eu ver se eu
copiei alguma coisa errada el teve um
bug j√° vamos ver se eu corrijo esse bug
ele colocou tio ele errou tamb√©m t√°
ent√£o j√° teve dois erros no c√≥digo
original eu vou corrigir isso para ele
vou tentar rodar com python V ver o que
acontece precisa de uma foto de carro
que ele acha que ele pediu pediu uma
foto de um carro aqui √≥ ter as imagens
carro obst√°culo dispon√≠veis no diret√≥rio
do projeto Ent√£o j√° tenho que fazer
coisas a mais aqui vamos fazer isso por
ele pegar uma fotinho num carro qualquer
aqui e vou fazer a mesma mesma coisa vou
s√≥ duplicar essa imagem e o carro vai
ser o obst√°culo tamb√©m beleza Botei as
duas imagens vamos ver se ele vai
funcionar agora √© teve muitos erros n√£o
rodou infelizmente ele n√£o passou nesse
teste Mas como eu falei esse teste n√£o
define nada √© apenas um teste existem
outros infinitos voc√™s podem testar por
a√≠ mas O legal √© que a gente j√° tem esse
modelo rodando localmente com essa com
esse mecanismo de pensamento eu posso
ver em tempo real aqui quanto o modelo
t√° consumindo da minha placa e um modelo
de 14 bilh√µes de par√¢metros est√° rodando
aqui consumiu 11 GB da minha placa se
voc√™s tiverem mais capacidade
computacional podem testar outras
varia√ß√µes aqui eu n√£o cheguei a testar
essa vers√£o de 1.5 de 7 B Mas as coisas
que eu quero fazer agora com isso √©
colocar esses modelos aqui dentro de
Agentes principalmente usando crei lchin
ver se modelos locais agora de fato v√£o
conseguir ter performance semelhante ao
que a gente tem tido com com 4 o Mas
isso √© assunto pros pr√≥ximos v√≠deos
==================================================

üîπ Finaliza√ß√£o
Ent√£o √© isso espero que tenham gostado
do v√≠deo tentei trazer um compilado de
muitas coisas que a gente observou nos
√∫ltimos dias sobre esse modelo espero
que voc√™s estejam t√£o excitados quanto
n√≥s estamos com esse tipo de tecnologia
se popularizando cada vez mais e eu sigo
esperando o momento aonde eu vou baixar
um modelo de 14 bilh√µes ou de 7 bilh√µes
aqui a performance vai ser semelhante ao
4 o que a gente tem hoje em dia esse dia
vai chegar galera se voc√™ quer aprender
mais sobre intelig√™ncia artificial
modelos de linguagem como rodar modelos
de linguagem locais e a como pegar as
apis de fato de cada um desses sistemas
que eu desenvolvi aqui construir
sistemas que de fato executam tarefas
por voc√™s ou seja agentes fica o meu
convite para clicar no link que t√°
aparecendo aqui na descri√ß√£o conhecer a
trilha de cursos aplica√ß√µes e ac com
python das zimov Academy que a gente vai
desde o zero na programa√ß√£o at√© Como
utilizar essas llms para construir esses
sistemas que s√£o o futuro um forte
abra√ßo a todos e at√© o pr√≥ximo v√≠deo
==================================================
